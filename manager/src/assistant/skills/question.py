def get_response_from_llm(model_data: dict, question: str):
    pass

def local_question(ws, question: str):
    prompt = "You are a helpful assistant. Answer the question: " + question
    return prompt
    
    # defer response + text response

def internet_question(ws, question: str):
    prompt = "You are a helpful assistant. Answer the question: " + question
    return prompt

    # defer response + text response
